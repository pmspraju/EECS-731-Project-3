{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EECS 731 Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import sklearn\n",
    "#import networkx as nx\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from pandas import compat\n",
    "import os\n",
    "#import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "#import nltk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns; sns.set()\n",
    "from textblob import TextBlob\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pre requisite functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function load the data from the given path and finename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path,filename):\n",
    "    try:\n",
    "             files = os.listdir(path)\n",
    "             for f in files:\n",
    "                 if f == filename:\n",
    "                     data = pd.read_csv(os.path.join(path,f))\n",
    "                     return data\n",
    "            \n",
    "    except Exception as ex:\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "           message = template.format(type(ex).__name__, ex.args)\n",
    "           print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function derives the shape of the dataset and returns the feature and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to explore the data\n",
    "def exploreData(data):\n",
    "    try:\n",
    "           #Total number of records                                  \n",
    "           rows = data.shape[0]\n",
    "           cols = data.shape[1]    \n",
    "           \n",
    "           #separate features and target\n",
    "           drop_col = ['rating']\n",
    "           features = data.drop(drop_col, axis = 1)\n",
    "           target = data[drop_col]\n",
    "          \n",
    "           # Print the results\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           print (\"Total number of records: {}\".format(rows))\n",
    "           print (\"Total number of features: {}\".format(cols))\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           \n",
    "           return features,target\n",
    "           \n",
    "    except Exception as ex:\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "           message = template.format(type(ex).__name__, ex.args)\n",
    "           print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function computes the percentage of missing values per each column in the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingValues(data):\n",
    "    try:\n",
    "           # Total missing values\n",
    "           mis_val = data.isnull().sum()\n",
    "         \n",
    "           # Percentage of missing values\n",
    "           mis_val_percent = 100 * mis_val / len(data)\n",
    "           \n",
    "           # Make a table with the results\n",
    "           mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "           \n",
    "           # Rename the columns\n",
    "           mis_val_table_ren_columns = mis_val_table.rename(\n",
    "           columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "           mis_val_table_ren_columns.head(4 )\n",
    "           # Sort the table by percentage of missing descending\n",
    "           misVal = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "                   '% of Total Values', ascending=False).round(1)\n",
    "                     \n",
    "           return misVal, mis_val_table_ren_columns\n",
    "\n",
    "    except Exception as ex:\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "           message = template.format(type(ex).__name__, ex.args)\n",
    "           print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method token the input string and return the sentiment polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentPolarity(sen):\n",
    "    try:\n",
    "        sp = TextBlob(sen)\n",
    "        if (sp < 0):\n",
    "            polarity = 0\n",
    "        else:\n",
    "            polarity = 1\n",
    "            \n",
    "        return polarity \n",
    "    except Exception as ex:\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "           message = template.format(type(ex).__name__, ex.args)\n",
    "           print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method scale the numerical features and label encoding for categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(data):\n",
    "    try:    \n",
    "        # TODO: One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
    "        #features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "        enc = LabelEncoder()\n",
    "        data['genre'] = enc.fit_transform(data['genre'])\n",
    "         \n",
    "        return data\n",
    "        \n",
    "    except Exception as ex:\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "           message = template.format(type(ex).__name__, ex.args)\n",
    "           print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method splits data in to training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(features, testsize):\n",
    "    try:\n",
    "        # Split the 'features' and 'income' data into training and testing sets\n",
    "        X_train, X_test = train_test_split(features,\n",
    "                                           test_size = testsize, \n",
    "                                           random_state = 1)\n",
    "\n",
    "        # Show the results of the split\n",
    "        print (\"Features training set has {} samples.\".format(X_train.shape[0]))\n",
    "        print (\"Features testing set has {} samples.\".format(X_test.shape[0]))\n",
    "        print (\"-----------------------------------------------------------------------\")\n",
    "        return X_train, X_test\n",
    "    except Exception as ex:\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "           message = template.format(type(ex).__name__, ex.args)\n",
    "           print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method implements the KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X_train, X_test):\n",
    "    try:\n",
    "       kmeans = KMeans(n_clusters=15, random_state=0).fit(X_train)\n",
    "       result = kmeans.predict(X_test)\n",
    "       scr = kmeans.score(X_test)\n",
    "       return result, scr\n",
    "    except Exception as ex:\n",
    "           print (\"-----------------------------------------------------------------------\")\n",
    "           template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "           message = template.format(type(ex).__name__, ex.args)\n",
    "           print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\pmspr\\Documents\\HS\\MS\\Sem 3\\EECS 731\\Week 5\\HW\\Git\\EECS-731-Project-3\\Data'\n",
    "filename = \"links.csv\"\n",
    "data_l = loadData(path,filename)\n",
    "\n",
    "path = r'C:\\Users\\pmspr\\Documents\\HS\\MS\\Sem 3\\EECS 731\\Week 5\\HW\\Git\\EECS-731-Project-3\\Data'\n",
    "filename = \"movies.csv\"\n",
    "data_m = loadData(path,filename)\n",
    "genres = ['Action','Adventure','Animation','Childrens','Comedy','Crime','Documentary','Drama','Fantasy','Film-Noir','Horror','Musical','Mystery','Romance','Sci-Fi','Thriller','War','Western']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create multiple records for each genre to perform clusters \n",
    "d1 = pd.DataFrame(columns = ['movieId','title','genre'])\n",
    "for ind, row in data_m.iterrows():\n",
    "    gstr = row['genres']\n",
    "    glst = gstr.split(\"|\")\n",
    "    cnt = 0\n",
    "    for x in glst:\n",
    "        d1.loc[ind + cnt] = data_m.loc[ind]\n",
    "        d1.at[ind+cnt,'genre'] = x\n",
    "        cnt = cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- Genres are in string format seprated by pipe\n",
    "- Create one record for each move with one genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load ratings data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\pmspr\\Documents\\HS\\MS\\Sem 3\\EECS 731\\Week 5\\HW\\Git\\EECS-731-Project-3\\Data'\n",
    "filename = \"ratings.csv\"\n",
    "data_r = loadData(path,filename)\n",
    "data_r = data_r.drop(['userId','timestamp'], axis = 1)\n",
    "data_r = data_r.groupby(['movieId'], as_index=False).max(level=0)\n",
    "data_r['rating'] = data_r.groupby(['movieId'], as_index=False)['rating'].apply(lambda x: x.value_counts().index[0])\n",
    "#data_r.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\pmspr\\Documents\\HS\\MS\\Sem 3\\EECS 731\\Week 5\\HW\\Git\\EECS-731-Project-3\\Data'\n",
    "filename = \"tags.csv\"\n",
    "data_t = loadData(path,filename)\n",
    "data_t = data_t.drop(['userId','timestamp'], axis = 1)\n",
    "data_t['tag'] = data_t['tag'].apply(lambda x: sentimentPolarity(x))\n",
    "data_t = data_t.groupby(['movieId'], as_index=False).count()\n",
    "#data_t.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- Use text blog to read the sentiment of each tag.\n",
    "- If sentiment polarity is 0, it has negative tendency and if 1, it is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.merge(data_r,data_t, on=['movieId'], how='inner')\n",
    "data = pd.merge(d1,data2, on=['movieId'], how='inner')\n",
    "data.to_csv('test.csv',index=False)\n",
    "\n",
    "drop_col = ['title']\n",
    "data = data.drop(drop_col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Shakespear Play data-----------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Total number of records: 1554\n",
      "Total number of features: 4\n",
      "-----------------------------------------------------------------------\n",
      "Columns that have missing values:0\n",
      "-----------------------------------------------------------------------\n",
      "         Missing Values  % of Total Values\n",
      "movieId  0               0.0              \n",
      "genre    0               0.0              \n",
      "rating   0               0.0              \n",
      "tag      0               0.0              \n",
      "Features training set has 1087 samples.\n",
      "Features testing set has 467 samples.\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"----------------------Shakespear Play data-----------------------------\")\n",
    "features, target = exploreData(data)\n",
    "misVal, mis_val_table_ren_columns = missingValues(data)\n",
    "# Print some summary information\n",
    "print (\"Columns that have missing values:\" + str(misVal.shape[0]))\n",
    "print (\"-----------------------------------------------------------------------\")\n",
    "print(mis_val_table_ren_columns.head(20))\n",
    "\n",
    "data_tran = transformData(data)\n",
    "X_train, X_test = splitData(data_tran,  0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- We can see no missing value in our curated dataset\n",
    "- We split our data in to train and test data with ratio of 70% training data and 30% test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement Cluster model Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  6  9  1  6  1  6  1  6  6  9  9  1  1  9  6  1  1  1  1  9  9  9 11\n",
      "  2  6  1  6  6  6  1  6 14  9  9  7  1  9  6  1  6  6  6  1  1  9  1  1\n",
      "  6  9  1  6  1  1  1  9  6  9  0  6  6  8  6  6  6  6 10  9  1  1  6  6\n",
      "  1  6  6  1  1  6  1  6 10  6  1  1 11  1  6  1  9  8  9  6  6  1  8  1\n",
      "  6  1  1  6 14 11  1  6  1  6  6  7  1 13 14  9  9  9  9 10 10  9  1  1\n",
      " 14  1  1  6  9  1  6 10  9  9 13  9  1  6  1  1  1  9  9 13  1  4  5  1\n",
      "  6  9  1  4  6  9  6  4  8  6  5  6  9  1  9  1  1  9  9  2  9 10  9  6\n",
      "  9  9  9  9  9  6 10  6  1  6  0  2  9  6  9  1  6  1  9  9  9  6  6  6\n",
      "  6  6  1  6  1 13  5  9  6  6  6  6  1  9  6  1  9  1  6  1  6  1  0  9\n",
      "  9  1  6  1 11  6 13  9  9  1  6  6  1  9  6  9  6  6  4  9  0  9  6  9\n",
      "  1  1  9 12  1  4  6  1  6 11  9  1  6 10  6  6  6  6  1  9  1  6  1  9\n",
      "  6  9  6  1  6  8  2  9  0  1  6  0  1  1  6  1  9  9  8  9 14  6  6  6\n",
      " 12  6  5  6  5  6  6  6  9  1  9  9  9  3  1  1  9  7  6  1  6  6  6  9\n",
      "  9  6  9  6  6  9  0 11  6  9  4  6  9  8  6  6  6  6  9  1  6  6  0  6\n",
      "  6  1  6  6  9  9  6  6  9  6 14  7  1  6  6  9  9  1  6  9  6  6  9  6\n",
      "  6  9  9  6 10  1  0  6  1  9  6  6  2  6  1 10  2  6  6  2 11  6  6  1\n",
      " 14  1 11  1  6  6  6  9 12  6  5  6  1  6  1  9  1  6  9  6  1  6  9 10\n",
      "  1 10  6  1  9  6  1  6  6  9  6  1  1  9  9  9  9  9  6  1  9  1  6  6\n",
      "  6  9  6  0 14  6  6 12  1  6  1  9  6  1  2 10  6  6  1  6  9  6  1  6\n",
      "  9  6  9  1  1  6  6  1  6  9 10]\n",
      "-1530976605.7688012\n"
     ]
    }
   ],
   "source": [
    "result,scr = kmeans(X_train, X_test)\n",
    "print(result)\n",
    "print(scr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- We can see different clusters in to which our test data is divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
